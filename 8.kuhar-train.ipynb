{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67537fc-f43d-42a7-9664-6dd17f8d9d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libhiaac.temporal.datasets.kuhar import BalancedTimeDomainKuHar\n",
    "from libhiaac.temporal.datasets.dataset import TemporalSample\n",
    "from libhiaac.temporal.transforms.transforms import Compose, SpectrogramCombiner\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()\n",
    "\n",
    "#val_dataset = BalancedTimeDomainKuHar(dataset_path=\"data/datasets/KuHar/Balanced\", download=True, mode=\"validation\")\n",
    "#test_dataset = BalancedTimeDomainKuHar(dataset_path=\"data/datasets/KuHar/Balanced\", download=True, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448a45d0-9d8d-43c3-ac71-22d74201ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeSpec:\n",
    "    def __call__(self, value):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.pcolormesh(value[1], value[0], value[2], shading=\"gouraud\", vmin=0, vmax=0.01)\n",
    "        fig.canvas.draw()\n",
    "        # Now we can save it to a numpy array.\n",
    "        data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(\"all\")\n",
    "        return data\n",
    "\n",
    "spec_combiner = SpectrogramCombiner(fs=100, nperseg=10, nooverlap=1)\n",
    "make_scpec_fig = MakeSpec()\n",
    "piller = torchvision.transforms.ToPILImage()\n",
    "resizer = torchvision.transforms.Resize((32,32))\n",
    "tensorize = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbeb7d73-79ab-4643-8837-9b92919e1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/datasets/KuHar/Balanced/train.csv' already exists and will not be downloaded again\n",
      "Done!\n",
      "'data/datasets/KuHar/Balanced/val.csv' already exists and will not be downloaded again\n",
      "Done!\n",
      "'data/datasets/KuHar/Balanced/test.csv' already exists and will not be downloaded again\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "composer = Compose([spec_combiner, make_scpec_fig, piller, resizer, tensorize])\n",
    "train_dataset = BalancedTimeDomainKuHar(dataset_path=\"data/datasets/KuHar/Balanced\", download=True, mode=\"train\", transforms=composer)\n",
    "val_dataset = BalancedTimeDomainKuHar(dataset_path=\"data/datasets/KuHar/Balanced\", download=True, mode=\"validation\", transforms=composer)\n",
    "test_dataset = BalancedTimeDomainKuHar(dataset_path=\"data/datasets/KuHar/Balanced\", download=True, mode=\"test\", transforms=composer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6929dc-e0d5-46ff-9123-fb36dc953538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:136: UserWarning: Legacy tensor constructor is deprecated. Use: torch.tensor(...) for creating tensors from tensor-like objects; or torch.empty(...) for creating an uninitialized tensor with specific sizes. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_new.cpp:478.)\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcd9779-8813-4766-8b25-823258840e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DenseNet in PyTorch.'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        \n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "        self.name = 'DenseNet' + str(sum(nblocks) * 2 + 5)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, num_classes=17)\n",
    "\n",
    "def DenseNet169():\n",
    "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32, num_classes=17)\n",
    "\n",
    "def DenseNet201():\n",
    "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32, num_classes=17)\n",
    "\n",
    "def DenseNet161():\n",
    "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48, num_classes=17)\n",
    "\n",
    "def densenet_cifar():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12, num_classes=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b24fc90-8406-499b-a67e-b0396833d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = DenseNet121()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb43cf32-7b51-4b32-a8d7-96a33349b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model = model#.to(\"cuda\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataset):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data.accelerometer\n",
    "            labels = torch.zeros(17)\n",
    "            labels[data.meta[\"class\"]] = 1\n",
    "            \n",
    "            \n",
    "            inputs =  torch.unsqueeze(inputs, dim=0)#.to(\"cuda\")\n",
    "            labels = torch.unsqueeze(labels, dim=0)#.to(\"cuda\")\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "                        \n",
    "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = correct/total\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0: \n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                print(f\"Acc={acc}\")\n",
    "                running_loss = 0.0\n",
    "        print(f'Epoch acc={acc}')\n",
    "\n",
    "    print(f'Finished Training: acc={acc}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee6b88-f56f-423c-ab58-157f55ae6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e7d4ce-a2c9-41ac-9407-42e122e4a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.030\n",
      "Acc=0.0\n",
      "[1,   101] loss: 0.116\n",
      "Acc=15.524752475247524\n",
      "[1,   201] loss: 0.684\n",
      "Acc=11.432835820895523\n",
      "[1,   301] loss: 1.171\n",
      "Acc=7.897009966777409\n",
      "[1,   401] loss: 0.000\n",
      "Acc=5.927680798004988\n",
      "[1,   501] loss: 1.489\n",
      "Acc=4.7445109780439125\n",
      "[1,   601] loss: 1.679\n",
      "Acc=3.9550748752079867\n",
      "[1,   701] loss: 1.739\n",
      "Acc=3.390870185449358\n",
      "[1,   801] loss: 0.000\n",
      "Acc=2.9675405742821472\n",
      "[1,   901] loss: 1.649\n",
      "Acc=2.6381798002219754\n",
      "[1,  1001] loss: 1.648\n",
      "Acc=2.3746253746253747\n",
      "[1,  1101] loss: 1.630\n",
      "Acc=2.1589464123524067\n",
      "[1,  1201] loss: 0.000\n",
      "Acc=1.9791840133222314\n",
      "[1,  1301] loss: 1.563\n",
      "Acc=1.8270561106840892\n",
      "[1,  1401] loss: 1.471\n",
      "Acc=1.6966452533904355\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-61c5e176c62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-660e2beccdd5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataset, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerometer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libhiaac/temporal/datasets/kuhar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0maccel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0maccel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0maccel_timestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0maccel_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporalSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccel_timestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Read gyroscope data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model = DenseNet121()\n",
    "train(train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8606d6-5e40-4d98-a4e5-72547df4a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.randint(0, 10, (10,)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
